{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543be75f-9ac2-4bd9-94ad-614579e7ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Mar 26 11:35:18 2024\n",
    "\n",
    "@author: Tanaka Akiyama\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to calculate daily averages\n",
    "def calculate_daily_averages(data):\n",
    "    # Number of hours in two weeks\n",
    "    hours_per_day = 24\n",
    "\n",
    "    # Reshape data to represent bi-weekly segments\n",
    "    data = data.reshape(-1, hours_per_day)\n",
    "\n",
    "    # Calculate bi-weekly averages\n",
    "    day_averages = np.mean(data, axis=1)\n",
    "    \n",
    "    return day_averages\n",
    "\n",
    "# Function to calculate bi-weekly averages\n",
    "def calculate_biweekly_averages(data):\n",
    "    # Number of hours in two weeks\n",
    "    hours_per_biweek = 24 * 14\n",
    "\n",
    "    # Determine the number of complete bi-weekly segments\n",
    "    num_biweeks = len(data) // hours_per_biweek\n",
    "\n",
    "    # Adjust the data to include only the necessary number of hours\n",
    "    data = data[:num_biweeks * hours_per_biweek]\n",
    "\n",
    "    # Reshape data to represent bi-weekly segments\n",
    "    data = data.reshape(-1, hours_per_biweek)\n",
    "\n",
    "    # Calculate bi-weekly averages\n",
    "    biweekly_averages = np.mean(data, axis=1)\n",
    "    \n",
    "    return biweekly_averages\n",
    "\n",
    "# Load NetCDF file\n",
    "def load_netcdf(file_path):\n",
    "    dataset = nc.Dataset(file_path)\n",
    "    return dataset\n",
    "\n",
    "# Main function\n",
    "def process_netcdf(file_paths, train_percent=0.93):\n",
    "    # Load NetCDF files for each year\n",
    "    datasets = [load_netcdf(file_path) for file_path in file_paths]\n",
    "    \n",
    "    # Initialize empty lists to store temperature and humidity data\n",
    "    temperatures = []\n",
    "    humidities = []\n",
    "    dates = []\n",
    "\n",
    "    # Loop through each dataset\n",
    "    for dataset in datasets:\n",
    "        latitude = dataset.variables['latitude'][:]\n",
    "        longitude = dataset.variables['longitude'][:]\n",
    "        time = dataset.variables['time'][:]\n",
    "        \n",
    "        # Extract the values corresponding to the third index (middle value)\n",
    "        middle_lat_index = len(latitude) // 2\n",
    "        middle_lon_index = len(longitude) // 2\n",
    "        temperature = dataset.variables['t'][:, middle_lat_index, middle_lon_index]\n",
    "        humidity = dataset.variables['r'][:, middle_lat_index, middle_lon_index]\n",
    "\n",
    "        # Extract scale and offset factors from attributes for temperature and humidity\n",
    "        temperature_scale = dataset.variables['t'].scale_factor\n",
    "        print(temperature_scale)\n",
    "        temperature_offset = dataset.variables['t'].add_offset\n",
    "        humidity_scale = dataset.variables['r'].scale_factor\n",
    "        humidity_offset = dataset.variables['r'].add_offset\n",
    "\n",
    "        # Apply scale and offset factors to temperature and humidity data\n",
    "        temperature = temperature * temperature_scale + temperature_offset\n",
    "        humidity = humidity * humidity_scale + humidity_offset\n",
    "\n",
    "        # Append temperature and humidity data to lists\n",
    "        temperatures.append(temperature)\n",
    "        humidities.append(humidity)\n",
    "        dates.append(time)\n",
    "        \n",
    "        dataset.close()\n",
    "\n",
    "    # Concatenate temperature and humidity data for all years into single arrays\n",
    "    all_temperature = np.concatenate(temperatures)\n",
    "    all_humidity = np.concatenate(humidities)\n",
    "    all_dates = np.concatenate(dates)\n",
    "    print(all_dates)\n",
    "\n",
    "    '''\n",
    "    # Calculate bi-weekly averages for temperature and humidity\n",
    "    temperature_biweekly = calculate_biweekly_averages(temperature)\n",
    "    humidity_biweekly = calculate_biweekly_averages(humidity)\n",
    "\n",
    "    # Create DataFrame with temperature, humidity, and index columns\n",
    "    df = pd.DataFrame({\n",
    "        'temperature': temperature_biweekly,\n",
    "        'humidity': humidity_biweekly,\n",
    "        'index': range(1, len(temperature_biweekly) + 1)  # 1 to 24\n",
    "    })\n",
    "\n",
    "    # Save DataFrame to CSV file\n",
    "    output_folder = 'processed'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    output_file = os.path.join(output_folder, 'processed_weather_data.csv')\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(\"Processed data saved to:\", output_file)\n",
    "    '''\n",
    "\n",
    "    # Calculate bi-weekly averages for temperature and humidity\n",
    "    temperature_daily = calculate_daily_averages(all_temperature)\n",
    "    humidity_daily = calculate_daily_averages(all_humidity)\n",
    "    dates_daily = calculate_daily_averages(all_dates)\n",
    "    print(all_dates.shape)\n",
    "    print(dates_daily.shape)\n",
    "\n",
    "    # Convert time from hours since 1900-01-01 00:00:00.0 to date\n",
    "    dates_daily = nc.num2date(dates_daily, \"hours since 1900-01-01 00:00:00.0\", \"gregorian\")\n",
    "\n",
    "    # Extract only the date portion from datetime objects\n",
    "    dates_daily = [datetime(d.year, d.month, d.day) for d in dates_daily]\n",
    "    \n",
    "    # Create DataFrame with temperature, humidity, and index columns\n",
    "    df = pd.DataFrame({\n",
    "        'date': dates_daily,\n",
    "        'temperature': temperature_daily,\n",
    "        'humidity': humidity_daily,\n",
    "        'index': range(1, len(temperature_daily) + 1) \n",
    "    })\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    train_size = int(len(df) * train_percent)\n",
    "    train_df = df.iloc[:train_size]\n",
    "    test_df = df.iloc[train_size:]\n",
    "\n",
    "    # Save train and test DataFrames to CSV files\n",
    "    output_folder = '../../data/processed'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    train_output_file = os.path.join(output_folder, 'train_weather_data_daily.csv')\n",
    "    test_output_file = os.path.join(output_folder, 'test_weather_data_daily.csv')\n",
    "    train_df.to_csv(train_output_file, index=False)\n",
    "    test_df.to_csv(test_output_file, index=False)\n",
    "    print(\"Train data saved to:\", train_output_file)\n",
    "    print(\"Test data saved to:\", test_output_file)\n",
    "    \n",
    "\n",
    "'''\n",
    "Prints netcdf file metadata.\n",
    "\n",
    "Parameters:\n",
    "file_path - string path to netcdf file\n",
    "'''\n",
    "def print_netcdf_metadata(file_path):\n",
    "    try:\n",
    "        dataset = nc.Dataset(file_path)\n",
    "        print(\"NetCDF file metadata:\")\n",
    "        print(\"Variables:\")\n",
    "        for var_name in dataset.variables:\n",
    "            var = dataset.variables[var_name]\n",
    "            print(\"\\tVariable name:\", var_name)\n",
    "            print(\"\\tDimensions:\", var.dimensions)\n",
    "            print(\"\\tShape:\", var.shape)\n",
    "            print(\"\\tUnits:\", var.units)\n",
    "            print(\"\\tAttributes:\")\n",
    "            for attr_name in var.ncattrs():\n",
    "                print(\"\\t\\t\", attr_name, \":\", getattr(var, attr_name))\n",
    "            print(\"\\n\")\n",
    "        print(\"Global attributes:\")\n",
    "        for attr_name in dataset.ncattrs():\n",
    "            print(\"\\t\", attr_name, \":\", getattr(dataset, attr_name))\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        \n",
    "\n",
    "\n",
    "# print_netcdf_metadata(netcdf_file_path)\n",
    "netcdf_file_paths = ['../../data/raw/weather/temp_relhum_2013.nc', '../../data/raw/weather/temp_relhum_2014.nc', \\\n",
    "                    '../../data/raw/weather/temp_relhum_2015.nc']\n",
    "\n",
    "# Example usage\n",
    "#process_netcdf(netcdf_file_paths)\n",
    "\n",
    "for file_path in netcdf_file_paths:\n",
    "    print_netcdf_metadata(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655aa094-9dba-4fa0-9860-cd1f0bb839a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
