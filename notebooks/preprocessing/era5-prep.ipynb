{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac60c9f-3cc4-4faa-ad1d-a60807b09bdf",
   "metadata": {},
   "source": [
    "# Pre-Processing Pipeline for ERA5 Dataset  \n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook contains the code for importing the raw ERA5 dataset and preparing it for use with a torchdiffeq and PyTorch implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e17e34-d446-4960-a82e-24b5c3419ace",
   "metadata": {},
   "source": [
    "## Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "543be75f-9ac2-4bd9-94ad-614579e7ce7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'netCDF4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mCreated on Tue Mar 26 11:35:18 2024\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m@author: Tanaka Akiyama and Gian Favero\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetCDF4\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnc\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'netCDF4'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Mar 26 11:35:18 2024\n",
    "\n",
    "@author: Tanaka Akiyama and Gian Favero\n",
    "\"\"\"\n",
    "\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to calculate daily averages\n",
    "def calculate_daily_average(data):\n",
    "    # Number of hours in a day\n",
    "    hours_per_day = 24\n",
    "\n",
    "    # Reshape data to represent daily segments\n",
    "    data = data.reshape(-1, hours_per_day)\n",
    "\n",
    "    # Calculate daily averages\n",
    "    day_averages = np.nanmean(data, axis=1)\n",
    "    \n",
    "    return day_averages\n",
    "\n",
    "# Function to calculate bi-weekly averages\n",
    "def calculate_biweekly_averages(data):\n",
    "    # Number of hours in two weeks\n",
    "    hours_per_biweek = 24 * 14\n",
    "\n",
    "    # Determine the number of complete bi-weekly segments\n",
    "    num_biweeks = len(data) // hours_per_biweek\n",
    "\n",
    "    # Adjust the data to include only the necessary number of hours\n",
    "    data = data[:num_biweeks * hours_per_biweek]\n",
    "\n",
    "    # Reshape data to represent bi-weekly segments\n",
    "    data = data.reshape(-1, hours_per_biweek)\n",
    "\n",
    "    # Calculate bi-weekly averages\n",
    "    biweekly_averages = np.mean(data, axis=1)\n",
    "    \n",
    "    return biweekly_averages\n",
    "\n",
    "# Load NetCDF file\n",
    "def load_netcdf(file_path):\n",
    "    dataset = nc.Dataset(file_path)\n",
    "    return dataset\n",
    "\n",
    "    \n",
    "'''\n",
    "Combines multiple years of data into the same dataframe\n",
    "'''\n",
    "def combine_variables(file_paths, average='biweekly'):\n",
    "    temps = []\n",
    "    humidities = []\n",
    "    times = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        # Load NetCDF file\n",
    "        dataset = load_netcdf(file_path)\n",
    "\n",
    "        # Extract temperature and humidity data\n",
    "        temperature_kelvin = dataset.variables['t'][:, 0, 0]  # Assuming only one latitude and longitude point\n",
    "        humidity = dataset.variables['r'][:, 0, 0]  # Assuming only one latitude and longitude point\n",
    "        time = dataset.variables['time'][:]\n",
    "        \n",
    "        ''' Seems like scales and offsets are already applied?\n",
    "        # Extract scale and offset factors from attributes for temperature and humidity\n",
    "        temperature_scale = dataset.variables['t'].scale_factor\n",
    "        temperature_offset = dataset.variables['t'].add_offset\n",
    "        humidity_scale = dataset.variables['r'].scale_factor\n",
    "        humidity_offset = dataset.variables['r'].add_offset\n",
    "\n",
    "        # Apply scale and offset factors to temperature and humidity data and convert to celcius\n",
    "        temperature = (temperature_kelvin * temperature_scale) + temperature_offset - 273.15\n",
    "        humidity = (humidity * humidity_scale + humidity_offset)\n",
    "        '''\n",
    "\n",
    "        #temperature = temperature_kelvin - 273.15\n",
    "        temperature = temperature_kelvin\n",
    "\n",
    "        # Set missing values to NaN\n",
    "        temperature[temperature == -32767] = np.nan\n",
    "        humidity[humidity == -32767] = np.nan\n",
    "\n",
    "        # Calculate averages\n",
    "        if average=='biweekly':\n",
    "            temp_average = calculate_biweekly_averages(temperature)\n",
    "            humidity_average = calculate_biweekly_averages(humidity)\n",
    "            time_average = calculate_biweekly_averages(time)\n",
    "        else:\n",
    "            temp_average = calculate_daily_average(temperature)\n",
    "            humidity_average = calculate_daily_average(humidity)\n",
    "            time_average = calculate_daily_average(time)\n",
    "\n",
    "        temps.append(temp_average)\n",
    "        humidities.append(humidity_average)\n",
    "        times.append(time_average)\n",
    "\n",
    "        dataset.close()\n",
    "\n",
    "    all_temperature = np.concatenate(temps)\n",
    "    all_humidity = np.concatenate(humidities)\n",
    "    all_dates = np.concatenate(times)\n",
    "\n",
    "    # Convert numeric time values to datetime objects\n",
    "    all_dates = nc.num2date(all_dates, units=\"hours since 1900-01-01 00:00:00.0\", calendar=\"gregorian\")\n",
    "    \n",
    "    # Extract only the date portion from datetime objects\n",
    "    all_dates = [datetime(d.year, d.month, d.day) for d in all_dates]\n",
    "\n",
    "    return all_temperature, all_humidity, all_dates, \n",
    "    \n",
    "\n",
    "# print_netcdf_metadata(netcdf_file_path)\n",
    "file_paths = ['../../data/raw/weather/temp_relhum_2013.nc', '../../data/raw/weather/temp_relhum_2014.nc', \\\n",
    "                    '../../data/raw/weather/temp_relhum_2015.nc', '../../data/raw/weather/temp_relhum_2016.nc', \\\n",
    "             '../../data/raw/weather/temp_relhum_2017.nc', '../../data/raw/weather/2018.nc', \\\n",
    "             '../../data/raw/weather/temp_relhum_2019.nc', '../../data/raw/weather/2020.nc', \\\n",
    "             '../../data/raw/weather/2021.nc', '../../data/raw/weather/2022.nc']\n",
    "\n",
    "temps, humidities, dates = combine_variables(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b748a146-e861-408d-accb-ce99677ddd78",
   "metadata": {},
   "source": [
    "## Save data to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "655aa094-9dba-4fa0-9860-cd1f0bb839a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data saved to: ../../data/processed\\train_weather_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Main function\n",
    "def save_to_csv(temps, humidities, dates, train_percent=0.93):\n",
    "    # Load NetCDF files for each year\n",
    "    datasets = [load_netcdf(file_path) for file_path in file_paths]\n",
    "    \n",
    "    # Create DataFrame with temperature, humidity, and index columns\n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'temperature': temps,\n",
    "        'humidity': humidities\n",
    "    })\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    train_size = int(len(df) * train_percent)\n",
    "    #train_df = df.iloc[:train_size]\n",
    "    #test_df = df.iloc[train_size:]\n",
    "\n",
    "    # Save train and test DataFrames to CSV files\n",
    "    output_folder = '../../data/processed'\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    train_output_file = os.path.join(output_folder, 'train_weather_data.csv')\n",
    "    #test_output_file = os.path.join(output_folder, 'test_weather_data.csv')\n",
    "    df.to_csv(train_output_file, index=False)\n",
    "    #test_df.to_csv(test_output_file, index=False)\n",
    "    print(\"Train data saved to:\", train_output_file)\n",
    "    #print(\"Test data saved to:\", test_output_file)\n",
    "\n",
    "save_to_csv(temps, humidities, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728d8a9-0562-426a-982d-5ff66e3d7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../../data/raw/ERA5/train_weather_data.csv')\n",
    "\n",
    "# Reshape to (10, 26, 2)\n",
    "data = data.drop(columns=['date'])\n",
    "data = data.to_numpy()\n",
    "data = data.reshape(10, 26, 2)\n",
    "\n",
    "# Normalize the data\n",
    "for i in range(10):\n",
    "    data[i, :, 0] = (data[i, :, 0] - np.mean(data[i, :, 0])) / np.std(data[i, :, 0])\n",
    "    data[i, :, 1] = (data[i, :, 1] - np.mean(data[i, :, 1])) / np.std(data[i, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43d1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time channel\n",
    "time = np.linspace(0, 25, 26)\n",
    "time = time.reshape(26, 1)\n",
    "time = np.tile(time, (10, 1, 1))\n",
    "data = np.concatenate((time, data), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7519fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensor\n",
    "data = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "# Split into train and test\n",
    "X_train = data[:8]\n",
    "X_test = data[8:]\n",
    "\n",
    "# Delete last 2 sequence elements from each sample\n",
    "X_train = X_train[:, :-2, :]\n",
    "X_test = X_test[:, :-2, :]\n",
    "\n",
    "# Save the data\n",
    "path = '../../data/processed/ERA5/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "torch.save(X_train, os.path.join(path, 'X_train.pt'))\n",
    "torch.save(X_test, os.path.join(path, 'X_test.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
